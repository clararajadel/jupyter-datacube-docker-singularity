{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_data (latitude, longitude, date, Rootoutput):\n",
    "    query = {\n",
    "        'product': 'B_Sentinel_2',\n",
    "        'output_crs': 'EPSG:32633',\n",
    "        'resolution': (-10, 10),\n",
    "        'lon': longitude,\n",
    "        'lat': latitude,\n",
    "        'time': date,\n",
    "        'measurements': ['B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B11', 'B12'], \n",
    "        'cloud_cover_percentage': (0.0, 50.0)\n",
    "    }\n",
    "    data = acube.load(**query)\n",
    "    data_array = np.array(data.to_array(), dtype='float')\n",
    "    for dataset in acube.find_datasets_lazy(**query):\n",
    "        geo = dataset.metadata_doc['extent']['coord']['ll']\n",
    "    return data_array, geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_selection_byCloud (fdate, latitude, longitude, cpercen, mode):\n",
    "    \n",
    "    '''\n",
    "    fdate = tuple(YYYY, MM, DD)\n",
    "    latitude = tuple(xx, yy)\n",
    "    longitude = tuple(xx, yy)\n",
    "    cpercen = str\n",
    "    mode = str('before') or str('after')\n",
    "    '''\n",
    "    \n",
    "    # date range\n",
    "    sdate =  fdate  # start date: flood date\n",
    "    if mode == 'before':\n",
    "        edate = date(2015, 7, 1)   # end date: first sentinel 2 image\n",
    "        delta = sdate - edate       # as timedelta\n",
    "        time = (str(edate), str(sdate)) # time range for query\n",
    "    elif mode == 'after':\n",
    "        now = datetime.datetime.now()\n",
    "        edate = date(now.year, now.month, now.day)  # end date: today\n",
    "        delta = edate - sdate       # as timedelta\n",
    "        time = (str(sdate), str(edate)) # time range for query\n",
    "    else:\n",
    "        print('No valid mode')\n",
    "    \n",
    "    # list available dates in acube: they are not ordered\n",
    "    c_product = 'CLOUDMASK_Sentinel_2'\n",
    "    c_query = {\n",
    "        'lat': latitude,\n",
    "        'lon': longitude,\n",
    "        'time': time\n",
    "    } \n",
    "    acube_dates = []\n",
    "    for dataset in acube.find_datasets_lazy(product=c_product, **c_query):\n",
    "        acube_dates.append((dataset.center_time).strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    # for loop per available days: see cloud coverage\n",
    "    for i in range(delta.days + 1):\n",
    "        if mode == 'before':\n",
    "            day = (sdate - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "        elif mode == 'after':\n",
    "            day = (sdate + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "        print('----------------------------------')\n",
    "        print('New Date:', day)\n",
    "        # query cloud data\n",
    "        if day in acube_dates:\n",
    "            print(f'day {day} is in acube_dates')\n",
    "            #print('Cloud data querying...')\n",
    "            cloud_query = {\n",
    "                'product': 'CLOUDMASK_Sentinel_2',\n",
    "                'output_crs': 'EPSG:32633',\n",
    "                'resolution': (-10, 10),\n",
    "                'lon': longitude,\n",
    "                'lat': latitude,\n",
    "                'time': day,\n",
    "                'measurements': ['band_1'],\n",
    "            }\n",
    "            # select image with cloud coverage condition\n",
    "            cloud = acube.load(**cloud_query)\n",
    "            #print('Transform query to array...')\n",
    "            cloud_array = np.array(cloud.to_array())[0] #there is only one band\n",
    "            print('Calculating cloud coverage...')\n",
    "            if len(np.unique(cloud_array[0])) == 1:\n",
    "                print('No cloud coverage')\n",
    "                s2_array, s2_geo = query_data(latitude, longitude, day, Rootoutput)\n",
    "                if not np.any(s2_array) == False:\n",
    "                    print('End')\n",
    "                    break\n",
    "                else:\n",
    "                    print('sentinel 2 band-arrays full of zeros!')\n",
    "                    continue\n",
    "            elif len(np.unique(cloud_array[0])) == 2:\n",
    "                cloud_coverage = np.count_nonzero(cloud_array[0] == 255)/(cloud_array.shape[1]*cloud_array.shape[2])*100\n",
    "                print('Cloud coverage (%):', cloud_coverage)\n",
    "                if cloud_coverage < cpercen:\n",
    "                    print(f'Cloud coverage less than {cpercen}%')\n",
    "                    s2_array, s2_geo = query_data(latitude, longitude, day, Rootoutput)\n",
    "                    if not np.any(s2_array) == False:\n",
    "                        print('End')\n",
    "                        break\n",
    "                    else:\n",
    "                        print('sentinel 2 band-arrays full of zeros!')\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f'Cloud coverage more than {cpercen}%')\n",
    "                    continue\n",
    "            else:\n",
    "                print('Error in cloud mask')\n",
    "        else:\n",
    "            print(f'day {day} is not in acube_dates')\n",
    "            continue\n",
    "    # delete temporal dimension (there is only one image)\n",
    "    s2_array = s2_array.reshape(s2_array.shape[0],s2_array.shape[2],s2_array.shape[3])\n",
    "    return day, s2_array, cloud_array, s2_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VegetationIndex(y1, filter_col, features):\n",
    "    # # Vegetation indices:\n",
    "    y_8bits = [((y1[i, :, :] - np.min(y1[i, :, :])) / (np.max(y1[i, :, :]) - np.min(y1[i, :, :]))) * 255 for i in\n",
    "               range(0, y1.shape[0])]\n",
    "\n",
    "    if 'NDVI' in features:\n",
    "        # NDVI veg_index <- (y[,nir]-y[,red])/(y[,nir]+y[,red])\n",
    "        ndvi = (y_8bits[filter_col.index('B8')] - y_8bits[filter_col.index('B4')]) / \\\n",
    "               (y_8bits[filter_col.index('B8')] + y_8bits[filter_col.index('B4')] + np.finfo(float).eps)\n",
    "        ndvi = np.reshape(ndvi, (ndvi.shape[0] * ndvi.shape[1], 1))\n",
    "        y1 = np.moveaxis(y1, 0, -1)\n",
    "        y1 = np.reshape(y1, (y1.shape[0] * y1.shape[1], y1.shape[2]))\n",
    "        y1 = np.concatenate((y1, ndvi), axis=1)\n",
    "\n",
    "    if 'NDWI' in features:\n",
    "        # NDWI <- (y[,green]-y[,nir])/(y[,green]+y[,nir])\n",
    "        ndwi = (y_8bits[filter_col.index('B3')] - y_8bits[filter_col.index('B8')]) / \\\n",
    "               (y_8bits[filter_col.index('B3')] + y_8bits[filter_col.index('B8')] + np.finfo(float).eps)\n",
    "        ndwi = np.reshape(ndwi, (ndwi.shape[0] * ndwi.shape[1], 1))\n",
    "        y1 = np.concatenate((y1, ndwi), axis=1)\n",
    "\n",
    "    if 'NDMI' in features:\n",
    "        # NDWI <- (y[,NIR]-y[,SWIR])/(y[,NIR]+y[,SWIR])\n",
    "        ndmi = (y_8bits[filter_col.index('B8')] - y_8bits[filter_col.index('B11')]) / \\\n",
    "               (y_8bits[filter_col.index('B8')] + y_8bits[filter_col.index('B11')] + np.finfo(float).eps)\n",
    "\n",
    "        ndmi = np.reshape(ndmi, (ndmi.shape[0] * ndmi.shape[1], 1))\n",
    "        y1 = np.concatenate((y1, ndwi), axis=1)\n",
    "\n",
    "    if 'EVI' in features:\n",
    "        # EVI veg_index <- 2.5*(y[,nir]-y[,red])/(y[,nir]+6*y[,red]-7.5*y[,blue]+1)\n",
    "        evi = 2.5 * (y_8bits[filter_col.index('B8')] - y_8bits[filter_col.index('B4')]) / \\\n",
    "              ((y_8bits[filter_col.index('B8')] + 6 * y_8bits[filter_col.index('B4')] - 7.5 *\n",
    "                y_8bits[filter_col.index('B2')] + 1) + np.finfo(float).eps)\n",
    "        evi = np.reshape(evi, (evi.shape[0] * evi.shape[1], 1))\n",
    "        y1 = np.concatenate((y1, evi), axis=1)\n",
    "\n",
    "    if 'TCARI' in features:\n",
    "        # TCARI veg_index <- 3*((y[,red_edge]-y[,red])-0.2*(y[,red_edge]-y[,green])*(y[,red_edge]/y[,red]))\n",
    "        tcari = 3 * (y_8bits[filter_col.index('B7')] - y_8bits[filter_col.index('B4')]) - \\\n",
    "                0.2 * (y_8bits[filter_col.index('B7')] - y_8bits[filter_col.index('B3')]) * (\n",
    "                        y_8bits[filter_col.index('B7')] / (y_8bits[filter_col.index('B4')] + np.finfo(float).eps))\n",
    "        tcari = np.reshape(tcari, (tcari.shape[0] * tcari.shape[1], 1))\n",
    "        y1 = np.concatenate((y1, tcari), axis=1)\n",
    "\n",
    "    if 'SAVI' in features:\n",
    "        # SAVI veg_index <- (1+L)*(y[,nir] - y[,red])/(y[,nir] + y[,red] + L) L=0.5\n",
    "        # savi = 1.5 * (y[7, :, :] - y[3, :, :]) / (y[7, :, :] + y[3, :, :] + 0.5)\n",
    "        savi = 1.5 * (y_8bits[filter_col.index('B8')] - y_8bits[filter_col.index('B4')]) / \\\n",
    "               ((y_8bits[filter_col.index('B8')] + y_8bits[filter_col.index('B4')] + 0.5) + np.finfo(float).eps)\n",
    "        savi = np.reshape(savi, (savi.shape[0] * savi.shape[1], 1))\n",
    "        y1 = np.concatenate((y1, savi), axis=1)\n",
    "\n",
    "    if 'KNDVI' in features:\n",
    "        # # kernel NDVI:\n",
    "        pr = [(y_8bits[i] - np.min(y_8bits[i])) / (np.max(y_8bits[i]) - np.min(y_8bits[i])) for i in\n",
    "              range(0, len(y_8bits))]\n",
    "        sigma_x = 0.15\n",
    "        # np.nanmedian(np.power(pr[filter_col.index('B8')] - pr[filter_col.index('B4')], 2))\n",
    "        ker = np.exp(\n",
    "            np.power(pr[filter_col.index('B8')] - pr[filter_col.index('B4')], 2) / (-2 * (np.power(sigma_x, 2))))\n",
    "        kndvi = (1 - ker) / (1 + ker)\n",
    "        kndvi = np.reshape(kndvi, (kndvi.shape[0] * kndvi.shape[1], 1))\n",
    "        y1 = np.concatenate((y1, kndvi), axis=1)\n",
    "\n",
    "        if 'MSAVI2' in features:\n",
    "            # MSAVI veg_index <- 0.5*(2*y[,nir]+1-sqrt(((2*y[,nir]+1)^2)-8*(y[,nir]-y[,red])))\n",
    "            # msavi2 = 0.5 * (2 * y[7, :, :] + 1 - np.sqrt(((2 * y[7, :, :] + 1) ** 2)\n",
    "            #                                              - 8 * (y[7, :, :] - y[3, :, :])))\n",
    "            msavi2 = 0.5 * (2 * y_8bits[filter_col.index('B8')] + 1 -\n",
    "                            np.sqrt(((2 * y_8bits[filter_col.index('B8')] + 1) ** 2) -\n",
    "                                    8 * (y_8bits[filter_col.index('B8')] - y_8bits[filter_col.index('B4')])))\n",
    "        msavi2 = np.reshape(msavi2, (msavi2.shape[0] * msavi2.shape[1], 1))\n",
    "        y1 = np.concatenate((y1, msavi2), axis=1)\n",
    "    return y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskGeneration(array, geo_inf, target_layer):\n",
    "# vector_layer = r\"C:\\Users\\Emma\\Desktop\\ACube4Floods\\code\\GT_data\\29_10_18_Gail.shp\"\n",
    "# raster_layer = r\"C:\\Users\\Emma\\Desktop\\ACube4Floods\\code\\data\\load_Carinthia\\clip\\Carinthia_clean_Image_20181117.tif\"\n",
    "# target_layer = r\"C:\\Users\\Emma\\Desktop\\ACube4Floods\\code\\mask.tif\"\n",
    "\n",
    "    # open the raster layer and get its relevant properties\n",
    "    xSize, ySize = array.shape\n",
    "    geotransform = ([geo_inf['lon'], 8.983152858765616e-05, 0.0, geo_ing['lat'], 0.0, -8.983152840909205e-05])\n",
    "    crs = 'EPSG:32633'\n",
    "\n",
    "    # create the target layer (1 band)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    target_ds = driver.Create(target_layer, xSize, ySize, bands = 1, eType = gdal.GDT_Byte, options = [\"COMPRESS=DEFLATE\"])\n",
    "    target_ds.SetGeoTransform(geotransform)\n",
    "    target_ds.SetProjection(crs)\n",
    "\n",
    "    # rasterize the vector layer into the target one\n",
    "    ds = gdal.Rasterize(target_ds, vector_layer, burnValues=[1])\n",
    "\n",
    "    target_ds = None\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGeoTiff(root_image):\n",
    "    tifsrc = gdal.Open(root_image)\n",
    "    in_band = tifsrc.GetRasterBand(1)\n",
    "    block_xsize, block_ysize = (in_band.XSize, in_band.YSize)\n",
    "    # read the multiband tile into a 3d numpy array\n",
    "    image = tifsrc.ReadAsArray(0, 0, block_xsize, block_ysize)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_VfoldCross(data, selec):\n",
    "    random.seed(30)\n",
    "\n",
    "    cls = np.unique(data)\n",
    "    arr_train = []\n",
    "\n",
    "    for i in cls:\n",
    "        #get the indexes for each\n",
    "        ind = np.where(data == i)\n",
    "\n",
    "        if len(ind[0]) < selec:\n",
    "            sel = random.sample(range(len(ind[0])), len(ind[0]))\n",
    "            sel = [sel[i] for i in range(int(np.round(2*len(ind[0])/3)))]\n",
    "            arr_train.extend(ind[0][sel])\n",
    "        else:\n",
    "            sel = random.sample(range(len(ind[0])), selec)\n",
    "            arr_train.extend(ind[0][sel])\n",
    "\n",
    "    return arr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestClassification(X_train, Y_train, n_feat, Njobs=None,\n",
    "                               vfolds=5, Ntree=[100], min_samples_lf=[1],\n",
    "                               min_samples_sp=[2]):\n",
    "\n",
    "    # CROSS-VALIDATION:\n",
    "    random.seed(999)\n",
    "    classifiers = []\n",
    "    cl = np.unique(Y_train)[0]\n",
    "    print('Cross validation...')\n",
    "    for ntree in Ntree:\n",
    "        for mtry in n_feat:\n",
    "            for lf in min_samples_lf:\n",
    "                for split in min_samples_sp:\n",
    "                    scores = []\n",
    "                    for t in range(0, vfolds):\n",
    "                        tr_index = ind_VfoldCross(Y_train, np.round(int(len(np.where(Y_train == cl)[0])/vfolds)))\n",
    "                        val_index = diff_emma(range(len(Y_train)), tr_index)\n",
    "                        x_t = X_train[tr_index, :]\n",
    "                        y_t = Y_train[tr_index]\n",
    "                        x_val = X_train[val_index, :]\n",
    "                        y_val = Y_train[val_index]\n",
    "                        clf = RandomForestClassifier(n_estimators=ntree, max_features=mtry, min_samples_leaf=lf,\n",
    "                                                     min_samples_split=split, n_jobs=Njobs)\n",
    "                        clf.fit(x_t, y_t)\n",
    "                        ypred = clf.predict(x_val)\n",
    "                        scores.append(accuracy_score(y_val, ypred))\n",
    "                    # print(scores)\n",
    "                    classifiers.append([ntree, mtry, lf, split, np.mean(scores)])\n",
    "                    # print(np.mean(scores))\n",
    "    classifiers = np.array(classifiers)\n",
    "    print('CV done!')\n",
    "    inx = np.where(classifiers == np.amax(classifiers, axis=0)[4])[0]\n",
    "    BestNtree = classifiers[inx, 0]\n",
    "    Bestn_feat = classifiers[inx, 1]\n",
    "    Bestmin_samples_lf = classifiers[inx, 2]\n",
    "    Bestmin_samples_sp = classifiers[inx, 3]\n",
    "    print('Training!')\n",
    "\n",
    "    cl_Final = RandomForestClassifier(n_estimators=int(BestNtree[0]), max_features=int(Bestn_feat[0]),\n",
    "                                      min_samples_leaf=int(Bestmin_samples_lf[0]),\n",
    "                                      min_samples_split=int(Bestmin_samples_sp[0]), n_jobs=Njobs)\n",
    "    cl_Final.fit(X_train, Y_train)\n",
    "    print('Predicting!')\n",
    "\n",
    "    # Ypred = cl_Final.predict(X_test)\n",
    "    # OA = accuracy_score(Y_test, Ypred)\n",
    "    #\n",
    "    # kappa = cohen_kappa_score(Y_test, Ypred)\n",
    "    #\n",
    "    # CM = confusion_matrix(Y_test, Ypred)\n",
    "    parameters = [BestNtree, Bestn_feat, Bestmin_samples_lf, Bestmin_samples_sp]\n",
    "    return cl_Final, parameters#OA, kappa, CM, Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_emma(first, second):\n",
    "    '''\n",
    "    returns \"first\" but deleting values included in \"second\"\n",
    "    '''\n",
    "    second = set(second)\n",
    "    return [item for item in first if item not in second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title,fontsize=20)\n",
    "    # plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45,fontsize=16)\n",
    "    plt.yticks(tick_marks, classes,fontsize=16)\n",
    " \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    " \n",
    "    print(cm)\n",
    " \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=14)\n",
    "    plt.xlabel('Predicted label',fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_save_results(CM, OA, kappa, PEN, SR, target_names, Rootoutput, identifier):\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(CM, classes=target_names, normalize=False, title=identifier, \n",
    "                          cmap=plt.cm.Blues)\n",
    "    plt.savefig(Rootoutput + identifier + '.png', format='png',dpi=1000, bbox_inches = \"tight\")\n",
    "    plt.close('all')\n",
    " \n",
    "    # save to disk as csv file\n",
    "    f = Rootoutput + identifier + '.csv'\n",
    "    \n",
    "    headerfile = identifier\n",
    " \n",
    "    #g = csv.writer(f, dialect='unix')\n",
    "    with open(f, 'w', newline='') as csvfile:\n",
    "        g = csv.writer(csvfile, delimiter=' ',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        g.writerow(headerfile)\n",
    "        g.writerow('')\n",
    "        g.writerow(['CM:'])\n",
    "        for i in range(CM.shape[0]):\n",
    "            g.writerow(CM[i,:])\n",
    "        g.writerow('')\n",
    "        g.writerow(['OA:', OA])\n",
    "        g.writerow('')\n",
    "        g.writerow(['Kappa:',kappa])\n",
    "        g.writerow('')\n",
    "        g.writerow(['PEN:', PEN])\n",
    "        g.writerow('')\n",
    "        g.writerow(['SR:',SR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X):\n",
    "    cols = []\n",
    "    descale = []\n",
    "    for feature in X.T:\n",
    "        minimum = feature.min(axis=0)\n",
    "        maximum = feature.max(axis=0)\n",
    "        col_std = np.divide((feature - minimum), (maximum - minimum))\n",
    "        cols.append(col_std)\n",
    "        descale.append((minimum, maximum))\n",
    "    X_std = np.array(cols)\n",
    "    return X_std.T, descale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeout(array, Rootoutput, identifier, ds_lon, ds_lat):\n",
    "    print(f'dtype: {array.dtype} to float64')\n",
    "    array = array.astype('float64')\n",
    "    cols = array.shape[-2]\n",
    "    rows = array.shape[-1]\n",
    "    # acube images are requested with this projection\n",
    "    crs = 'EPSG:32633'\n",
    "    #(xmin, xsize, 0, ymin, 0, ysize)\n",
    "    geotransform = ([ds_lon, 8.983152858765616e-05, 0.0, ds_lat, 0.0, -8.983152840909205e-05])\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    # acube images are uint16\n",
    "    ds = driver.Create(Rootoutput + identifier + '.tif', rows, cols, 1, gdal.GDT_Float64 )\n",
    "    ds.SetGeoTransform(geotransform)\n",
    "    ds.SetProjection(crs)\n",
    "    outband=ds.GetRasterBand(1)\n",
    "    outband.WriteArray(array)\n",
    "    ds = None\n",
    "    outband = None\n",
    "    print('exported')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (randomforest)",
   "language": "python",
   "name": "randomforest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
